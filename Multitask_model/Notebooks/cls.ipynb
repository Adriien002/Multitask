{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patches for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Chemin du dossier parent\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Exemple d'import\n",
    "import config\n",
    "import utils\n",
    "\n",
    "import data.dataset as dataset\n",
    "import data.transform as T_mtsk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> TaskBasedTransform initialized\n",
      ">>> TaskBasedTransform initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Projet_Hemorragie/hemorragie-env/lib/python3.12/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "train_data=dataset.get_classification_data(split=\"train\")\n",
    "train_transforms, val_transforms = T_mtsk.TaskBasedTransform_V2(keys=[\"image\", \"label\"]), T_mtsk.TaskBasedValTransform_V2(keys=[\"image\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur du jeu de données: 1274\n",
      " : clé de l'image : dict_keys(['image', 'label', 'task'])\n",
      " : type de l'image : <class 'str'>\n",
      "label : [1. 0. 1. 1. 1. 0.]\n",
      "<class 'dict'>\n",
      "torch.Size([1, 96, 96, 96])\n",
      "metatensor([1., 0., 1., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "print (f\"longueur du jeu de données: {len(train_data)}\")\n",
    "sample = train_data[0]   # on récupère le premier échantillon\n",
    "print(f\" : clé de l'image : {sample.keys()}\") \n",
    "print(f\" : type de l'image : {type(sample['image'])}\")\n",
    "print(f\"label : {sample['label']}\")\n",
    "\n",
    "transformed_sample = train_transforms(sample)\n",
    "\n",
    "# Vérifier le résultat\n",
    "print(type(transformed_sample))\n",
    "print(transformed_sample[\"image\"].shape)\n",
    "print(transformed_sample[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from monai.transforms import MapTransform\n",
    "from monai.utils.misc import ensure_tuple_rep\n",
    "from typing import Dict, Hashable, Sequence, Union\n",
    "\n",
    "\n",
    "class SlidingWindowCropd(MapTransform):\n",
    "    \"\"\"\n",
    "    Transformation MONAI pour créer des patches en sliding window.\n",
    "    Compatible avec les pipelines MONAI existantes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: Union[Hashable, Sequence[Hashable]],\n",
    "        roi_size: Union[int, Sequence[int]],\n",
    "        overlap: float = 0.0,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            keys: clés à transformer (ex: [\"image\"])\n",
    "            roi_size: taille des patches (ex: (96, 96, 96))\n",
    "            overlap: pourcentage de chevauchement entre patches (0.0 = pas de chevauchement)\n",
    "            allow_missing_keys: permettre les clés manquantes\n",
    "        \"\"\"\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.roi_size = ensure_tuple_rep(roi_size, 3)  # Assure 3D\n",
    "        self.overlap = overlap\n",
    "        \n",
    "    def _compute_patch_positions(self, image_shape: Sequence[int]) -> list:\n",
    "        \"\"\"Calcule les positions de départ de chaque patch\"\"\"\n",
    "        positions = []\n",
    "        \n",
    "        # Pour chaque dimension\n",
    "        starts = []\n",
    "        for i, (img_size, patch_size) in enumerate(zip(image_shape[-3:], self.roi_size)):\n",
    "            if img_size <= patch_size:\n",
    "                # Si l'image est plus petite que le patch, un seul patch centré\n",
    "                starts.append([max(0, (img_size - patch_size) // 2)])\n",
    "            else:\n",
    "                # Calcule le stride avec overlap\n",
    "                stride = int(patch_size * (1 - self.overlap))\n",
    "                stride = max(1, stride)  # Au moins 1\n",
    "                \n",
    "                start_positions = []\n",
    "                start = 0\n",
    "                while start + patch_size <= img_size:\n",
    "                    start_positions.append(start)\n",
    "                    start += stride\n",
    "                \n",
    "                print(f\"start:{start}\")\n",
    "                print(f\"start_pos{start_positions}\")\n",
    "                # Assure qu'on couvre bien la fin de l'image\n",
    "                if start_positions[-1] + patch_size < img_size:\n",
    "                  \n",
    "                    \n",
    "                    start_positions.append(img_size - patch_size)\n",
    "                # starts liste de liste ou chaque liste contient points de départ patch selon dimension\n",
    "                starts.append(start_positions) \n",
    "                \n",
    "        print(f\"starts at the end : {starts}\")\n",
    "        # Génère toutes les combinaisons de positions\n",
    "        for z_start in starts[0]:\n",
    "            for y_start in starts[1]:\n",
    "                for x_start in starts[2]:\n",
    "                    positions.append((z_start, y_start, x_start))\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    def _extract_patches(self, image: torch.Tensor, positions: list) -> torch.Tensor:\n",
    "        \"\"\"Extrait tous les patches de l'image\"\"\"\n",
    "        patches = []\n",
    "        \n",
    "        for z_start, y_start, x_start in positions:\n",
    "            z_end = z_start + self.roi_size[0]\n",
    "            y_end = y_start + self.roi_size[1]\n",
    "            x_end = x_start + self.roi_size[2]\n",
    "            \n",
    "            patch = image[..., z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "            patches.append(patch)\n",
    "        \n",
    "        # Stack tous les patches : [N_patches, C, H, W, D]\n",
    "        return torch.stack(patches, dim=0)\n",
    "    \n",
    "    def __call__(self, data: Dict) -> Dict:\n",
    "        d = dict(data)\n",
    "        \n",
    "        for key in self.key_iterator(d):\n",
    "            image = d[key]\n",
    "            \n",
    "            # Assure que c'est un tensor\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                image = torch.tensor(image)\n",
    "            \n",
    "            # Calcule les positions des patches\n",
    "            positions = self._compute_patch_positions(image.shape)\n",
    "            \n",
    "            # Extrait les patches\n",
    "            patches = self._extract_patches(image, positions)\n",
    "            \n",
    "            # Stocke les patches et les métadonnées\n",
    "            d[key] = patches\n",
    "            d[f\"{key}_patch_positions\"] = positions\n",
    "            d[f\"{key}_original_shape\"] = image.shape\n",
    "            d[f\"{key}_n_patches\"] = len(positions)\n",
    "        \n",
    "        return d\n",
    "\n",
    "\n",
    "# Exemple d'utilisation dans ta pipeline de classification\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Test rapide\n",
    "#     import torch\n",
    "    \n",
    "#     # Simule une image 3D\n",
    "#     fake_data = {\n",
    "#         \"image\": torch.randn(1, 128, 128, 96),  # [C, H, W, D]\n",
    "#         \"task\": \"classification\"\n",
    "#     }\n",
    "    \n",
    "#     # Applique la transformation\n",
    "#     sliding_window = SlidingWindowCropd(\n",
    "#         keys=[\"image\"], \n",
    "#         roi_size=(96, 96, 96), \n",
    "#         overlap=0.2\n",
    "#     )\n",
    "    \n",
    "#     result = sliding_window(fake_data)\n",
    "    \n",
    "#     print(f\"Original shape: {fake_data['image'].shape}\")\n",
    "#     print(f\"Patches shape: {result['image'].shape}\")\n",
    "#     print(f\"Number of patches: {result['image_n_patches']}\")\n",
    "#     print(f\"Patch positions: {result['image_patch_positions'][:3]}...\")  # Premiers patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils.misc import ensure_tuple_rep\n",
    "# test ensure_tuple_rep\n",
    "ensure_tuple_rep((1,1,1), 3)\n",
    "\n",
    "\n",
    "#test compute_patch_position\n",
    "fake_data = {\n",
    "        \"image\": torch.randn(1, 128, 128, 96),  # [C, H, W, D]\n",
    "        \"task\": \"classification\",\n",
    "        \"label\": [1,0,0,1,0,1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial dimensions of image: (512, 512, 128)\n",
      "Patch dimensions (roi_size): (96, 96, 96)\n",
      "\n",
      "Zipped dimensions (iterator): [(512, 96), (512, 96), (128, 96)]\n",
      "\n",
      "--- Iteration 0 ---\n",
      "  Current dimension index (i): 0\n",
      "  Image size for this dimension (img_size): 512\n",
      "  Patch size for this dimension (patch_size): 96\n",
      "  This corresponds to: \n",
      "    The first spatial dimension (e.g., Depth)\n",
      "\n",
      "--- Iteration 1 ---\n",
      "  Current dimension index (i): 1\n",
      "  Image size for this dimension (img_size): 512\n",
      "  Patch size for this dimension (patch_size): 96\n",
      "  This corresponds to: \n",
      "    The second spatial dimension (e.g., Height)\n",
      "\n",
      "--- Iteration 2 ---\n",
      "  Current dimension index (i): 2\n",
      "  Image size for this dimension (img_size): 128\n",
      "  Patch size for this dimension (patch_size): 96\n",
      "  This corresponds to: \n",
      "    The third spatial dimension (e.g., Width)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_shape = (1, 512, 512, 128)\n",
    "roi_size = (96, 96, 96)\n",
    "\n",
    "# We are only interested in the last 3 dimensions\n",
    "spatial_dims_shape = image_shape[-3:] # -> (128, 128, 96)\n",
    "print(f\"Spatial dimensions of image: {spatial_dims_shape}\")\n",
    "print(f\"Patch dimensions (roi_size): {roi_size}\\n\")\n",
    "\n",
    "# Let's see what zip() does\n",
    "zipped_dimensions = zip(spatial_dims_shape, roi_size)\n",
    "print(f\"Zipped dimensions (iterator): {list(zipped_dimensions)}\\n\")\n",
    "# After converting to a list for printing, the iterator is exhausted.\n",
    "\n",
    "# Let's restart the zip to show what happens in the loop\n",
    "zipped_dimensions_for_loop = zip(spatial_dims_shape,roi_size)\n",
    "for i, (img_size, patch_size) in enumerate(zipped_dimensions_for_loop):\n",
    "    print(f\"--- Iteration {i} ---\")\n",
    "    print(f\"  Current dimension index (i): {i}\")\n",
    "    print(f\"  Image size for this dimension (img_size): {img_size}\")\n",
    "    print(f\"  Patch size for this dimension (patch_size): {patch_size}\")\n",
    "    print(f\"  This corresponds to: \")\n",
    "    if i == 0:\n",
    "        print(\"    The first spatial dimension (e.g., Depth)\")\n",
    "    elif i == 1:\n",
    "        print(\"    The second spatial dimension (e.g., Height)\")\n",
    "    elif i == 2:\n",
    "        print(\"    The third spatial dimension (e.g., Width)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test sans chevauchement (overlap=0.0) ---\n",
      "start:480\n",
      "start_pos[0, 96, 192, 288, 384]\n",
      "start:480\n",
      "start_pos[0, 96, 192, 288, 384]\n",
      "start:96\n",
      "start_pos[0]\n",
      "starts at the end : [[0, 96, 192, 288, 384, 416], [0, 96, 192, 288, 384, 416], [0, 32]]\n",
      "Forme de l'image: (512, 512, 128)\n",
      "Taille du patch: (96, 96, 96)\n",
      "Nombre total de patches: 72\n",
      "Positions de départ des patches:\n",
      "[(0, 0, 0), (0, 0, 32), (0, 96, 0), (0, 96, 32), (0, 192, 0), (0, 192, 32), (0, 288, 0), (0, 288, 32), (0, 384, 0), (0, 384, 32), (0, 416, 0), (0, 416, 32), (96, 0, 0), (96, 0, 32), (96, 96, 0), (96, 96, 32), (96, 192, 0), (96, 192, 32), (96, 288, 0), (96, 288, 32), (96, 384, 0), (96, 384, 32), (96, 416, 0), (96, 416, 32), (192, 0, 0), (192, 0, 32), (192, 96, 0), (192, 96, 32), (192, 192, 0), (192, 192, 32), (192, 288, 0), (192, 288, 32), (192, 384, 0), (192, 384, 32), (192, 416, 0), (192, 416, 32), (288, 0, 0), (288, 0, 32), (288, 96, 0), (288, 96, 32), (288, 192, 0), (288, 192, 32), (288, 288, 0), (288, 288, 32), (288, 384, 0), (288, 384, 32), (288, 416, 0), (288, 416, 32), (384, 0, 0), (384, 0, 32), (384, 96, 0), (384, 96, 32), (384, 192, 0), (384, 192, 32), (384, 288, 0), (384, 288, 32), (384, 384, 0), (384, 384, 32), (384, 416, 0), (384, 416, 32), (416, 0, 0), (416, 0, 32), (416, 96, 0), (416, 96, 32), (416, 192, 0), (416, 192, 32), (416, 288, 0), (416, 288, 32), (416, 384, 0), (416, 384, 32), (416, 416, 0), (416, 416, 32)]\n",
      "\n",
      "--- Test avec chevauchement (overlap=0.25) ---\n",
      "start:432\n",
      "start_pos[0, 72, 144, 216, 288, 360]\n",
      "start:432\n",
      "start_pos[0, 72, 144, 216, 288, 360]\n",
      "start:72\n",
      "start_pos[0]\n",
      "starts at the end : [[0, 72, 144, 216, 288, 360, 416], [0, 72, 144, 216, 288, 360, 416], [0, 32]]\n",
      "Forme de l'image: (512, 512, 128)\n",
      "Taille du patch: (96, 96, 96)\n",
      "Nombre total de patches: 98\n",
      "Positions de départ des patches:\n",
      "[(0, 0, 0), (0, 0, 32), (0, 72, 0), (0, 72, 32), (0, 144, 0), (0, 144, 32), (0, 216, 0), (0, 216, 32), (0, 288, 0), (0, 288, 32), (0, 360, 0), (0, 360, 32), (0, 416, 0), (0, 416, 32), (72, 0, 0), (72, 0, 32), (72, 72, 0), (72, 72, 32), (72, 144, 0), (72, 144, 32), (72, 216, 0), (72, 216, 32), (72, 288, 0), (72, 288, 32), (72, 360, 0), (72, 360, 32), (72, 416, 0), (72, 416, 32), (144, 0, 0), (144, 0, 32), (144, 72, 0), (144, 72, 32), (144, 144, 0), (144, 144, 32), (144, 216, 0), (144, 216, 32), (144, 288, 0), (144, 288, 32), (144, 360, 0), (144, 360, 32), (144, 416, 0), (144, 416, 32), (216, 0, 0), (216, 0, 32), (216, 72, 0), (216, 72, 32), (216, 144, 0), (216, 144, 32), (216, 216, 0), (216, 216, 32), (216, 288, 0), (216, 288, 32), (216, 360, 0), (216, 360, 32), (216, 416, 0), (216, 416, 32), (288, 0, 0), (288, 0, 32), (288, 72, 0), (288, 72, 32), (288, 144, 0), (288, 144, 32), (288, 216, 0), (288, 216, 32), (288, 288, 0), (288, 288, 32), (288, 360, 0), (288, 360, 32), (288, 416, 0), (288, 416, 32), (360, 0, 0), (360, 0, 32), (360, 72, 0), (360, 72, 32), (360, 144, 0), (360, 144, 32), (360, 216, 0), (360, 216, 32), (360, 288, 0), (360, 288, 32), (360, 360, 0), (360, 360, 32), (360, 416, 0), (360, 416, 32), (416, 0, 0), (416, 0, 32), (416, 72, 0), (416, 72, 32), (416, 144, 0), (416, 144, 32), (416, 216, 0), (416, 216, 32), (416, 288, 0), (416, 288, 32), (416, 360, 0), (416, 360, 32), (416, 416, 0), (416, 416, 32)]\n"
     ]
    }
   ],
   "source": [
    "transform_no_overlap = SlidingWindowCropd(keys=[\"image\"], roi_size=(96, 96, 96), overlap=0.0)\n",
    "transform_with_overlap = SlidingWindowCropd(keys=[\"image\"], roi_size=(96, 96, 96), overlap=0.25)\n",
    "\n",
    "# Définir la forme de l'image que vous voulez tester\n",
    "image_shape = (1, 512, 512, 128) # [C, H, W, D]\n",
    "\n",
    "# Tester la méthode _compute_patch_positions sans chevauchement\n",
    "print(\"--- Test sans chevauchement (overlap=0.0) ---\")\n",
    "positions_no_overlap = transform_no_overlap._compute_patch_positions(image_shape)\n",
    "print(f\"Forme de l'image: {image_shape[1:]}\")\n",
    "print(f\"Taille du patch: {transform_no_overlap.roi_size}\")\n",
    "print(f\"Nombre total de patches: {len(positions_no_overlap)}\")\n",
    "print(f\"Positions de départ des patches:\\n{positions_no_overlap}\\n\")\n",
    "\n",
    "# Tester la méthode _compute_patch_positions avec chevauchement\n",
    "print(\"--- Test avec chevauchement (overlap=0.25) ---\")\n",
    "positions_with_overlap = transform_with_overlap._compute_patch_positions(image_shape)\n",
    "print(f\"Forme de l'image: {image_shape[1:]}\")\n",
    "print(f\"Taille du patch: {transform_with_overlap.roi_size}\")\n",
    "print(f\"Nombre total de patches: {len(positions_with_overlap)}\")\n",
    "print(f\"Positions de départ des patches:\\n{positions_with_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: /home/tibia/Projet_Hemorragie/MBH_label_case/ID_45be38b3_ID_fa1ed92647.nii.gz\n",
      "Dimensions of the first image: (512, 512, 27)\n"
     ]
    }
   ],
   "source": [
    "import data.dataset as dataset\n",
    "\n",
    "import nibabel as nib\n",
    "import data.dataset as dataset\n",
    "import os\n",
    "\n",
    "# Assuming your get_classification_data function is correctly defined and imports necessary modules.\n",
    "\n",
    "# Get the list of data dictionaries\n",
    "train_data = dataset.get_classification_data(split=\"train\")\n",
    "\n",
    "# Check if the list is not empty\n",
    "if train_data:\n",
    "    # Get the dictionary for the first image\n",
    "    first_image_dict = train_data[9]\n",
    "    \n",
    "    # Get the image file path from the dictionary\n",
    "    image_path = first_image_dict[\"image\"]\n",
    "    \n",
    "    # Check if the file exists before trying to load it\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            # Load the NIfTI file using nibabel\n",
    "            img_obj = nib.load(image_path)\n",
    "            \n",
    "            # Get the data dimensions from the NIfTI object\n",
    "            image_dimensions = img_obj.shape\n",
    "            \n",
    "            print(f\"File path: {image_path}\")\n",
    "            print(f\"Dimensions of the first image: {image_dimensions}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image file {image_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {image_path}\")\n",
    "else:\n",
    "    print(\"No data found in the train dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/img/ID_0237f3c9_ID_40015688b9.nii.gz\n",
      "Dimensions of the first segmentation image: (512, 512, 27)\n",
      "File path: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/img/ID_02b882cc_ID_a4892e60ae.nii.gz\n",
      "Dimensions of the first segmentation image: (512, 512, 30)\n",
      "File path: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/img/ID_02f779fb_ID_c4d7f33559.nii.gz\n",
      "Dimensions of the first segmentation image: (512, 512, 31)\n",
      "File path: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/img/ID_041e6601_ID_1fa1d20bba.nii.gz\n",
      "Dimensions of the first segmentation image: (512, 512, 32)\n",
      "File path: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/img/ID_0492041f_ID_b4fcf2f799.nii.gz\n",
      "Dimensions of the first segmentation image: (512, 512, 36)\n",
      "\n",
      "--- Segmentation Train Dataset Summary ---\n",
      "Number of images with 22 slices: 2\n",
      "Number of images with 23 slices: 1\n",
      "Number of images with 25 slices: 3\n",
      "Number of images with 26 slices: 8\n",
      "Number of images with 27 slices: 6\n",
      "Number of images with 28 slices: 9\n",
      "Number of images with 29 slices: 12\n",
      "Number of images with 30 slices: 21\n",
      "Number of images with 31 slices: 17\n",
      "Number of images with 32 slices: 17\n",
      "Number of images with 33 slices: 3\n",
      "Number of images with 34 slices: 12\n",
      "Number of images with 35 slices: 6\n",
      "Number of images with 36 slices: 13\n",
      "Number of images with 37 slices: 5\n",
      "Number of images with 38 slices: 10\n",
      "Number of images with 39 slices: 2\n",
      "Number of images with 42 slices: 2\n",
      "Number of images with 44 slices: 2\n",
      "Number of images with 46 slices: 2\n",
      "Number of images with 58 slices: 1\n",
      "-----------------------------------\n",
      "\n",
      "--- Classification Train Dataset Summary ---\n",
      "Number of images with 23 slices: 3\n",
      "Number of images with 24 slices: 4\n",
      "Number of images with 25 slices: 2\n",
      "Number of images with 26 slices: 45\n",
      "Number of images with 27 slices: 49\n",
      "Number of images with 28 slices: 52\n",
      "Number of images with 29 slices: 47\n",
      "Number of images with 30 slices: 148\n",
      "Number of images with 31 slices: 112\n",
      "Number of images with 32 slices: 129\n",
      "Number of images with 33 slices: 92\n",
      "Number of images with 34 slices: 95\n",
      "Number of images with 35 slices: 84\n",
      "Number of images with 36 slices: 82\n",
      "Number of images with 37 slices: 77\n",
      "Number of images with 38 slices: 65\n",
      "Number of images with 39 slices: 63\n",
      "Number of images with 40 slices: 49\n",
      "Number of images with 41 slices: 13\n",
      "Number of images with 42 slices: 18\n",
      "Number of images with 43 slices: 12\n",
      "Number of images with 44 slices: 7\n",
      "Number of images with 45 slices: 3\n",
      "Number of images with 46 slices: 6\n",
      "Number of images with 47 slices: 4\n",
      "Number of images with 48 slices: 1\n",
      "Number of images with 49 slices: 2\n",
      "Number of images with 50 slices: 1\n",
      "Number of images with 51 slices: 3\n",
      "Number of images with 52 slices: 2\n",
      "Number of images with 53 slices: 1\n",
      "Number of images with 54 slices: 1\n",
      "Number of images with 55 slices: 1\n",
      "Number of images with 56 slices: 1\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import data.dataset as dataset\n",
    "import config\n",
    "import nibabel as nib\n",
    "import data.dataset as dataset\n",
    "import os\n",
    "\n",
    "seg_data=dataset.get_segmentation_data(split = 'train')\n",
    "cls_data=dataset.get_classification_data(split = 'train')\n",
    "\n",
    "for i in range(5):\n",
    "    image_dict= seg_data[i]\n",
    "    image_path= image_dict[\"image\"]\n",
    "    img_obj = nib.load(image_path)\n",
    "    image_dimensions = img_obj.shape\n",
    "    print(f\"File path: {image_path}\")\n",
    "\n",
    "    print(f\"Dimensions of the first segmentation image: {image_dimensions}\")\n",
    "    \n",
    "import nibabel as nib\n",
    "import os\n",
    "import collections\n",
    "import data.dataset as dataset\n",
    "from typing import Dict, List\n",
    "\n",
    "def count_images_by_slices(dataset: List[Dict], dataset_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Counts and prints the number of images per slice count for a given dataset.\n",
    "    \"\"\"\n",
    "    slice_counts = collections.defaultdict(int)\n",
    "    \n",
    "    # Iterate through each dictionary in the dataset list\n",
    "    for data_dict in dataset:\n",
    "        image_path = data_dict.get(\"image\")\n",
    "        if not image_path or not os.path.exists(image_path):\n",
    "            print(f\"Warning: Image file not found at {image_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load the image and get its dimensions\n",
    "            img_obj = nib.load(image_path)\n",
    "            # The number of slices is the last dimension in the (D, H, W) tuple\n",
    "            num_slices = img_obj.shape[-1]\n",
    "            slice_counts[num_slices] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "    # Print the results in a readable format\n",
    "    print(f\"\\n--- {dataset_name} Dataset Summary ---\")\n",
    "    if not slice_counts:\n",
    "        print(\"No images were successfully processed.\")\n",
    "        return\n",
    "        \n",
    "    for num_slices, count in sorted(slice_counts.items()):\n",
    "        print(f\"Number of images with {num_slices} slices: {count}\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "# --- Main execution ---\n",
    "seg_data = dataset.get_segmentation_data(split='train')\n",
    "cls_data = dataset.get_classification_data(split='train')\n",
    "\n",
    "count_images_by_slices(seg_data, \"Segmentation Train\")\n",
    "count_images_by_slices(cls_data, \"Classification Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Image Metadata ---\n",
      "File Name: /home/tibia/Projet_Hemorragie/Seg_hemorragie/split_MONAI/train/img/ID_0237f3c9_ID_40015688b9.nii.gz\n",
      "Affine Matrix:\n",
      "tensor([[-4.8828e-01,  0.0000e+00,  0.0000e+00,  1.2500e+02],\n",
      "        [ 0.0000e+00, -4.6168e-01, -1.7216e+00,  1.6214e+02],\n",
      "        [ 0.0000e+00, -1.5897e-01,  4.9999e+00,  2.5580e+01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],\n",
      "       dtype=torch.float64)\n",
      "affine shape: torch.Size([4, 4])\n",
      "Original Spatial Shape: [512 512  27]\n",
      "Header:\n",
      "[[-4.88281012e-01  0.00000000e+00  0.00000000e+00  1.25000000e+02]\n",
      " [ 0.00000000e+00 -4.61678816e-01 -1.72160287e+00  1.62141907e+02]\n",
      " [ 0.00000000e+00 -1.58968604e-01  4.99990287e+00  2.55799370e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nibabel as nib\n",
    "import monai.transforms as T\n",
    "from monai.data import decollate_batch\n",
    "import os\n",
    "import data.dataset as dataset\n",
    "import config\n",
    "\n",
    "# Use the first image from your dataset\n",
    "seg_data = dataset.get_segmentation_data(split='train')\n",
    "image_dict = seg_data[0]\n",
    "\n",
    "# Define a simple transform pipeline to load the image\n",
    "simple_transform = T.Compose([\n",
    "    T.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    T.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "# Apply the transform to the single image dictionary\n",
    "transformed_data = simple_transform(image_dict)\n",
    "\n",
    "# The loaded data is a MetaTensor. You can access its metadata.\n",
    "image_meta = transformed_data['image'].meta\n",
    "\n",
    "# Print the metadata\n",
    "print(\"--- Image Metadata ---\")\n",
    "print(f\"File Name: {image_meta.get('filename_or_obj')}\")\n",
    "print(f\"Affine Matrix:\\n{image_meta.get('affine')}\")\n",
    "print(f\"affine shape: {image_meta.get('affine').shape}\")\n",
    "print(f\"Original Spatial Shape: {image_meta.get('spatial_shape')}\")\n",
    "print(f\"Header:\\n{image_meta.get('original_affine')}\")\n",
    "print(\"-\" * 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hemorragie-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
